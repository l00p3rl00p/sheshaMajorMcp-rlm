# Architecture & Development Guide

## 1. Tactile Workflow (How to Work)

### Build & Install
Local development requires Python 3.11+.

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install editable package with dev dependencies
pip install -e ".[dev]"
```

### Verification Loops
We use a strict quality gate. Run `make all` before committing.

```bash
# Run all tests
pytest

# Fast iteration on single test
pytest tests/path/test_name.py -v

# Static Analysis
mypy src/shesha       # Type checking
ruff check src tests  # Linting
ruff format src tests # Auto-formatting
```

### Mandatory TDD Protocol
**ALL CODE MUST USE TEST-DRIVEN DEVELOPMENT. NO EXCEPTIONS.**

1. **RED**: Write a failing test FIRST. Run it. Confirm failure.
2. **GREEN**: Write MINIMAL code to pass. Nothing extra.
3. **REFACTOR**: Clean up if needed, keeping tests green.
4. **COMMIT**: After each green state.

**Violations:** Implementation before tests, skipping red phase, writing more code than needed.

### Release Workflow
Version numbers are derived from git tags via `hatch-vcs`.

1. Move `[Unreleased]` entries in `CHANGELOG.md` to a new `[X.Y.Z] - YYYY-MM-DD` section.
2. Add a fresh `## [Unreleased]` section at the top.
3. Commit the changelog update.
4. Tag and push:
   ```bash
   git tag vX.Y.Z
   git push && git push --tags
   ```

---

## 2. Logic Model (How it Works)

### Core Loop
Shesha implements a Recursive Language Model (RLM) architecture where the LLM writes code to explore documents in a loop.

```text
User Query → RLM Core Loop → Docker Sandbox
                ↓                  ↓
           LiteLLM Client    llm_query() sub-calls
                ↓
           Trace Recorder
```

### Key Components
- **RLM Engine** (`src/shesha/rlm/`): Manages the core query loop, context handling, and stopping criteria (`FINAL("answer")`).
- **Sandbox** (`src/shesha/sandbox/`): Docker-based secure execution environment. Manages a pool of hot containers for low latency.
- **Storage** (`src/shesha/storage/`): Abstraction for document persistence.
- **Librarian** (`src/shesha/librarian/`): CLI and MCP server wrapper for tool interoperability.

### Key Design Decisions
- **Sub-LLM Depth = 1**: To ensure predictable costs and latency, recursion is limited to one level deep (plain LLM call, not full RLM recursion).
- **Max Iterations = 20**: Hard limit to prevent infinite loops (configurable).
- **Hot Container Pool**: Maintains 3 warm containers to eliminate startup overhead during the core loop.
- **Project-Based Organization**: Documents are grouped into named projects for logical separation.
- **Graceful Docker Degradation**: Shesha initializes successfully even if Docker is unavailable. Query functionality is disabled until Docker is present, with clear error messages guiding users.
- **Unified GUI Stack**: The Bridge server serves the production GUI directly from `gui/dist`. This eliminates the need for separate dev servers and simplifies deployment for operators.
- **Production-Ready GUI Mechanics**: The GUI is a fully bundled React application with zero external runtime dependencies.
  - **Local Tailwind**: CSS is compiled locally using Tailwind v3 and PostCSS.
  - **Self-Hosted Assets**: Critical assets like Inter, JetBrains Mono, and Space Grotesk fonts are bundled locally.
  - **Zero CDN**: No reliance on external CDNs (React, Lucide, Tailwind, or Google Fonts) at runtime.
  - **Offline-First**: Designed to function 100% offline (excluding external avatar APIs).
  - **Smart Launcher**: The `librarian gui` command includes a 10s retry logic to wait for the Bridge to initialize, providing a seamless "one-click" experience.

### System Requirements & Installation
**Design Philosophy**: Infrastructure dependencies should be detected and validated before installation proceeds.

- **System Audit**: The installer (`librarian install`) performs pre-flight checks:
  - Python version validation (requires 3.11+, exits with clear error if older)
  - Virtual environment detection (warns if running globally, provides setup guidance)
  - Defensive error handling for unexpected platform issues

### Docker Availability Handling
**Design Philosophy**: Docker is an infrastructure dependency, not a fatal requirement at startup.

- **Detection**: `Shesha._is_docker_available()` checks Docker availability without raising errors.
- **Conditional Initialization**: The `ContainerPool` is only created if Docker is detected.
- **Clear Feedback**: When queries are attempted without Docker, users receive actionable error messages.
- **Interactive Setup**: The Librarian CLI installer (`librarian install`) detects missing Docker and provides:
  - Platform-specific installation guides (Docker Desktop, Homebrew, Colima)
  - Options to skip Docker (queries disabled) or abort installation
  - Manifest tracking of Docker status for operational transparency

---

## 3. Security & Constraints

### Threat Model
The system executes arbitrary code generated by an LLM. This is inherently risky.

### Sandbox Isolation
- **Docker**: All generated code runs in a minimal Docker container.
- **Network**: Containers are network-isolated with an egress whitelist strictly for LLM APIs (if needed by the code).
- **Resource Limits**: Containers have strict memory and CPU quotas.

### Untrusted Content Handling
To prevent prompt injection:
- **`llm_query(instruction, content)`**: `instruction` is treated as trusted (system prompt), while `content` is wrapped in `<untrusted_document_content>` tags.
- **REPL Output**: Any output from the sandbox is wrapped in `<repl_output type="untrusted_document_content">` tags before being fed back to the LLM.

### GitHub Copilot
This repo uses `.github/copilot-instructions.md` to align AI suggestions with our TDD and security rules.

---

## 4. Codebase Map

### Directory Structure

```text
src/shesha/
├── shesha.py          # Main entry point - creates projects, manages lifecycle
├── project.py         # Project class - upload docs, run queries
├── config.py          # SheshaConfig - loads from env/file/args
├── models.py          # Data classes (ParsedDocument, RepoProjectResult, etc.)
├── exceptions.py      # Exception hierarchy
│
├── rlm/               # The RLM engine (core loop)
│   ├── engine.py      # RLMEngine.query() - the REPL+LLM loop
│   ├── prompts.py     # System prompts, output wrapping, security tags
│   └── trace.py       # Execution tracing (steps, tokens, timing)
│
├── sandbox/           # Docker-based code execution
│   ├── pool.py        # ContainerPool - keeps warm containers ready
│   ├── executor.py    # ContainerExecutor - runs code, handles llm_query
│   ├── runner.py      # Runs INSIDE the container (receives JSON commands)
│   └── Dockerfile     # The sandbox image
│
├── parser/            # Document parsing
│   ├── base.py        # DocumentParser protocol
│   ├── registry.py    # ParserRegistry - finds parser by file type
│   ├── code.py        # Source code (with line numbers, shebang detection)
|   └── ...            # Other format parsers (text, pdf, office, html)
│
├── storage/           # Document persistence
│   ├── base.py        # StorageBackend protocol
│   └── filesystem.py  # FilesystemStorage implementation
│
├── repo/              # Git repository ingestion
│   └── ingester.py    # Clone, pull, list files, track SHA
│
└── llm/               # LLM client
    └── client.py      # LLMClient - wraps LiteLLM
```

### Where to Look Guide

- **Query Logic**: `rlm/engine.py` (main loop)
- **Prompt Engineering**: `rlm/prompts.py` (system prompts, security tags)
- **Document Support**: `parser/` (add new `DocumentParser` implementations)
- **Sandbox Execution**: `sandbox/runner.py` (inside container) or `sandbox/executor.py` (host side)
- **Container Lifecycle**: `sandbox/pool.py` (warm pool management)
- **Storage Layer**: `storage/filesystem.py` (how files are saved)
- **Git Operations**: `repo/ingester.py` (cloning, SHA tracking)
- **Public API**: `shesha.py` or `project.py`
- **Configuration**: `config.py` (env var handling)
