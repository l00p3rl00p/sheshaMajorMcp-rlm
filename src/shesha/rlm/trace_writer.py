"""Trace writer for saving query traces to JSONL files."""

import datetime
import json
import logging
from pathlib import Path

from shesha.models import QueryContext
from shesha.rlm.trace import TokenUsage, Trace, TraceStep
from shesha.security.redaction import redact
from shesha.storage.filesystem import FilesystemStorage

logger = logging.getLogger(__name__)


class TraceWriter:
    """Writes query traces to JSONL files."""

    def __init__(self, storage: FilesystemStorage) -> None:
        """Initialize with storage backend."""
        self.storage = storage

    def write_trace(
        self,
        project_id: str,
        trace: Trace,
        context: QueryContext,
        answer: str,
        token_usage: TokenUsage,
        execution_time: float,
        status: str,
    ) -> Path | None:
        """Write a trace to a JSONL file.

        Args:
            project_id: The project ID
            trace: The execution trace
            context: Query metadata
            answer: The final answer
            token_usage: Token usage statistics
            execution_time: Total execution time in seconds
            status: Query status (success, max_iterations, error)

        Returns:
            Path to the created trace file, or None if write failed
        """
        try:
            # Apply redaction
            redacted_trace = trace.redacted()

            traces_dir = self.storage.get_traces_dir(project_id)

            # Generate filename: YYYY-MM-DDTHH-MM-SS-mmm_shortid.jsonl
            now = datetime.datetime.now(datetime.UTC)
            timestamp = now.strftime("%Y-%m-%dT%H-%M-%S") + f"-{now.microsecond // 1000:03d}"
            short_id = context.trace_id[:8]
            filename = f"{timestamp}_{short_id}.jsonl"

            # Build lines
            lines: list[str] = []

            # Header
            header = {
                "type": "header",
                "trace_id": context.trace_id,
                "timestamp": now.isoformat(),
                "question": context.question,
                "document_ids": context.document_ids,
                "model": context.model,
                "system_prompt": context.system_prompt,
                "subcall_prompt": context.subcall_prompt,
            }
            lines.append(json.dumps(header))

            # Steps (use redacted trace)
            for step in redacted_trace.steps:
                step_data = {
                    "type": "step",
                    "step_type": step.type.value,
                    "iteration": step.iteration,
                    "timestamp": datetime.datetime.fromtimestamp(
                        step.timestamp, tz=datetime.UTC
                    ).isoformat(),
                    "content": step.content,
                    "tokens_used": step.tokens_used,
                    "duration_ms": step.duration_ms,
                }
                lines.append(json.dumps(step_data))

            # Summary
            max_iteration = max((s.iteration for s in redacted_trace.steps), default=0)
            summary = {
                "type": "summary",
                "answer": answer,
                "total_iterations": max_iteration + 1,
                "total_tokens": {
                    "prompt": token_usage.prompt_tokens,
                    "completion": token_usage.completion_tokens,
                },
                "total_duration_ms": int(execution_time * 1000),
                "status": status,
            }
            lines.append(json.dumps(summary))

            # Write file
            path = traces_dir / filename
            path.write_text("\n".join(lines) + "\n")

            return path

        except Exception as e:
            logger.warning(f"Failed to write trace for project {project_id}: {e}")
            return None

    def cleanup_old_traces(self, project_id: str, max_count: int = 50) -> None:
        """Remove oldest traces if count exceeds max_count.

        Args:
            project_id: The project ID
            max_count: Maximum number of traces to keep
        """
        traces = self.storage.list_traces(project_id)
        if len(traces) <= max_count:
            return

        # Sorted oldest first, delete until under limit
        to_delete = traces[: len(traces) - max_count]
        for trace_path in to_delete:
            trace_path.unlink()


class IncrementalTraceWriter:
    """Writes trace data incrementally as steps happen.

    Unlike TraceWriter which writes the entire trace at the end,
    this writer appends each step to disk as it occurs. This ensures
    partial traces are available even if the process is interrupted.
    """

    def __init__(self, storage: FilesystemStorage) -> None:
        """Initialize with storage backend."""
        self.storage = storage
        self.path: Path | None = None
        self._max_iteration: int = 0

    def start(self, project_id: str, context: QueryContext) -> Path | None:
        """Create trace file and write the header line.

        Args:
            project_id: The project ID.
            context: Query metadata.

        Returns:
            Path to the created file, or None if creation failed.
        """
        try:
            traces_dir = self.storage.get_traces_dir(project_id)
            now = datetime.datetime.now(datetime.UTC)
            timestamp = now.strftime("%Y-%m-%dT%H-%M-%S") + f"-{now.microsecond // 1000:03d}"
            short_id = context.trace_id[:8]
            filename = f"{timestamp}_{short_id}.jsonl"

            self.path = traces_dir / filename

            header = {
                "type": "header",
                "trace_id": context.trace_id,
                "timestamp": now.isoformat(),
                "question": context.question,
                "document_ids": context.document_ids,
                "model": context.model,
                "system_prompt": context.system_prompt,
                "subcall_prompt": context.subcall_prompt,
            }
            self.path.write_text(json.dumps(header) + "\n")
            return self.path

        except Exception as e:
            logger.warning(f"Failed to start incremental trace for project {project_id}: {e}")
            self.path = None
            return None

    def write_step(self, step: TraceStep) -> None:
        """Append a single step to the trace file.

        Args:
            step: The trace step to write.
        """
        if self.path is None:
            return

        try:
            self._max_iteration = max(self._max_iteration, step.iteration)
            step_data = {
                "type": "step",
                "step_type": step.type.value,
                "iteration": step.iteration,
                "timestamp": datetime.datetime.fromtimestamp(
                    step.timestamp, tz=datetime.UTC
                ).isoformat(),
                "content": redact(step.content),
                "tokens_used": step.tokens_used,
                "duration_ms": step.duration_ms,
            }
            with self.path.open("a") as f:
                f.write(json.dumps(step_data) + "\n")
        except Exception as e:
            logger.warning(f"Failed to write incremental trace step: {e}")

    def finalize(
        self,
        answer: str,
        token_usage: TokenUsage,
        execution_time: float,
        status: str,
    ) -> None:
        """Append summary line to the trace file.

        Args:
            answer: The final answer (or partial state).
            token_usage: Token usage statistics.
            execution_time: Total execution time in seconds.
            status: Query status (success, max_iterations, interrupted).
        """
        if self.path is None:
            return

        try:
            summary = {
                "type": "summary",
                "answer": answer,
                "total_iterations": self._max_iteration + 1,
                "total_tokens": {
                    "prompt": token_usage.prompt_tokens,
                    "completion": token_usage.completion_tokens,
                },
                "total_duration_ms": int(execution_time * 1000),
                "status": status,
            }
            with self.path.open("a") as f:
                f.write(json.dumps(summary) + "\n")
        except Exception as e:
            logger.warning(f"Failed to finalize incremental trace: {e}")
